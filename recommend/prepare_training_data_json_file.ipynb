{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b2f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Handles serialization of common numpy datatypes\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "def md(markdown: str):\n",
    "    display(Markdown(markdown))\n",
    "\n",
    "\n",
    "def pprint(obj):\n",
    "    md(f\"```json\\n{json.dumps(obj, indent=2, cls=NpEncoder)}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d077de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# Assuming the target directory is one level up from the current working directory\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import your custom package\n",
    "import draco \n",
    "from draco import dict_to_facts\n",
    "from draco.data_utils import pairs_to_vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5eb7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clingo\n",
    "\n",
    "def parse_fact_to_symbol(fact_str):\n",
    "    # Parses a string fact and returns a clingo.Symbol\n",
    "    return clingo.parse_term(fact_str.rstrip('.'))\n",
    "\n",
    "def convert_list_to_symbols(fact_list):\n",
    "    # Converts a list of string facts to an Iterable[Symbol]\n",
    "    return [parse_fact_to_symbol(fact) for fact in fact_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b88617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dicts(a, b):\n",
    "    \"\"\"Compare two dictionaries, treating lists as unordered.\"\"\"\n",
    "    \n",
    "    if not isinstance(a, dict) or not isinstance(b, dict) or len(a) != len(b):\n",
    "        return False\n",
    "    \n",
    "    for key in a:\n",
    "        if key not in b:\n",
    "            return False\n",
    "        \n",
    "        if isinstance(a[key], list) and isinstance(b[key], list):\n",
    "            if not compare_lists(a[key], b[key]):\n",
    "                return False\n",
    "        elif isinstance(a[key], dict) and isinstance(b[key], dict):\n",
    "            if not compare_dicts(a[key], b[key]):\n",
    "                return False\n",
    "        else:\n",
    "            if a[key] != b[key]:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def compare_lists(a, b):\n",
    "    \"\"\"Compare two lists, ignoring order.\"\"\"\n",
    "    if len(a) != len(b):\n",
    "        return False\n",
    "    \n",
    "    a_sorted = sorted(a, key=custom_sort_key)\n",
    "    b_sorted = sorted(b, key=custom_sort_key)\n",
    "    \n",
    "    for a_item, b_item in zip(a_sorted, b_sorted):\n",
    "        if isinstance(a_item, dict) and isinstance(b_item, dict):\n",
    "            if not compare_dicts(a_item, b_item):\n",
    "                return False\n",
    "        else:\n",
    "            if a_item != b_item:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def custom_sort_key(item):\n",
    "    \"\"\"Generate a sort key for items that can be dictionaries.\"\"\"\n",
    "    if isinstance(item, dict):\n",
    "        return tuple(sorted((k, custom_sort_key(v)) for k, v in item.items()))\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65aec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how to \n",
    "check = '[\"attribute(task,root,value).\", \"attribute(number_rows,root,30).\", \"entity(field,root,0).\", \"attribute((field,name),0,n).\", \"attribute((field,type),0,string).\", \"attribute((field,entropy),0,1000).\", \"attribute((field,unique),0,10).\", \"entity(field,root,1).\", \"attribute((field,name),1,q1).\", \"attribute((field,type),1,number).\", \"attribute((field,entropy),1,1305).\", \"attribute((field,unique),1,30).\", \"attribute((field,interesting),1,true).\", \"entity(field,root,2).\", \"attribute((field,name),2,q2).\", \"attribute((field,type),2,number).\", \"attribute((field,entropy),2,1299).\", \"attribute((field,unique),2,30).\", \"entity(view,root,3).\", \"attribute((view,coordinates),3,cartesian).\", \"entity(mark,3,4).\", \"attribute((mark,type),4,point).\", \"entity(encoding,4,5).\", \"attribute((encoding,channel),5,x).\", \"attribute((encoding,field),5,n).\", \"entity(encoding,4,6).\", \"attribute((encoding,channel),6,y).\", \"attribute((encoding,field),6,q2).\", \"entity(encoding,4,7).\", \"attribute((encoding,channel),7,color).\", \"attribute((encoding,field),7,q1).\", \"entity(scale,3,8).\", \"attribute((scale,channel),8,x).\", \"attribute((scale,type),8,ordinal).\", \"entity(scale,3,9).\", \"attribute((scale,channel),9,y).\", \"attribute((scale,type),9,).\", \"entity(scale,3,10).\", \"attribute((scale,channel),10,color).\", \"attribute((scale,type),10,linear).\", \"attribute((scale,zero),10,true).\"], \"negative\": [\"attribute(task,root,value).\", \"attribute(number_rows,root,30).\", \"entity(field,root,0).\", \"attribute((field,name),0,n).\", \"attribute((field,type),0,string).\", \"attribute((field,entropy),0,1000).\", \"attribute((field,unique),0,10).\", \"entity(field,root,1).\", \"attribute((field,name),1,q1).\", \"attribute((field,type),1,number).\", \"attribute((field,entropy),1,1305).\", \"attribute((field,unique),1,30).\", \"attribute((field,interesting),1,true).\", \"entity(field,root,2).\", \"attribute((field,name),2,q2).\", \"attribute((field,type),2,number).\", \"attribute((field,entropy),2,1299).\", \"attribute((field,unique),2,30).\", \"entity(view,root,3).\", \"attribute((view,coordinates),3,cartesian).\", \"entity(mark,3,4).\", \"attribute((mark,type),4,point).\", \"entity(encoding,4,5).\", \"attribute((encoding,channel),5,size).\", \"attribute((encoding,field),5,q1).\", \"entity(encoding,4,6).\", \"attribute((encoding,channel),6,y).\", \"attribute((encoding,field),6,q2).\", \"entity(encoding,4,7).\", \"attribute((encoding,channel),7,x).\", \"attribute((encoding,field),7,n).\", \"entity(scale,3,8).\", \"attribute((scale,channel),8,size).\", \"attribute((scale,type),8,linear).\", \"attribute((scale,zero),8,true).\", \"entity(scale,3,9).\", \"attribute((scale,channel),9,y).\", \"attribute((scale,type),9,linear).\", \"attribute((scale,zero),9,true).\", \"entity(scale,3,10).\", \"attribute((scale,channel),10,x).\", \"attribute((scale,type),10,ordinal).\"]'\n",
    "check = check.replace('\"', \"'\")\n",
    "\n",
    "def write_dicts_to_json_training_file(model, temperature):\n",
    "    with open(training_data_path, 'w') as training_file:\n",
    "        training_file.write(\"[\\n\")\n",
    "\n",
    "\n",
    "    removed, err = 0, 0\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(optimal_completion_dir)):\n",
    "\n",
    "        underscore_index = filename.find('_')\n",
    "        if underscore_index == -1:\n",
    "            raise ValueError(\"no _ found\")\n",
    "        num = filename[:underscore_index]\n",
    "\n",
    "        with open(os.path.join(original_pairs_dir, f\"{num}_pos_first.txt\"), 'r') as file:\n",
    "            dicts_to_compare = []\n",
    "            pairs = []\n",
    "            for line in file:\n",
    "                #check if line is empty line. correct code\n",
    "                if line == \"\\n\":\n",
    "                    continue\n",
    "                chart = line.strip()  # Remove leading/trailing whitespace, including newlines\n",
    "                data_facts = json.loads(chart[9:])[\"field\"]\n",
    "                pairs.append(dict_to_facts(json.loads(chart[9:])))\n",
    "                dicts_to_compare.append(json.loads(chart[9:]))\n",
    "#                 print(json.loads(chart[9:]))\n",
    "#                 return\n",
    "    #             break\n",
    "\n",
    "        if filename.endswith('.txt'):\n",
    "            filename = os.path.join(optimal_completion_dir, filename)\n",
    "\n",
    "            with open(filename, 'r') as file:\n",
    "                content = file.read()\n",
    "                if content == \"error\":\n",
    "                    err += 1\n",
    "                    continue\n",
    "\n",
    "                task = None\n",
    "                if \"value\" in content:\n",
    "                    task = \"value\"\n",
    "                elif \"summary\" in content:\n",
    "                    task = \"summary\"\n",
    "                else:\n",
    "                    raise ValueError(\"wrong task type\")\n",
    "                    \n",
    "               \n",
    "                    \n",
    "                content_json = json.loads(content)\n",
    "#                 print(type(content_json), pairs[0])\n",
    "                content_json[\"field\"] = data_facts\n",
    "                \n",
    "                positive_facts = dict_to_facts(content_json)\n",
    "#                 flag = False\n",
    "#                 if {'channel': 'x', 'field': 'q1'} in content_json[\"view\"][0][\"mark\"][0][\"encoding\"] and\\\n",
    "#                     {'channel': 'y', 'field': 'q2'} in content_json[\"view\"][0][\"mark\"][0][\"encoding\"] and\\\n",
    "#                     {'channel': 'color', 'field': 'n'} in content_json[\"view\"][0][\"mark\"][0][\"encoding\"]:\n",
    "#                     flag = True\n",
    "# #                     print(content_json[\"view\"][0][\"mark\"][0][\"encoding\"])\n",
    "# #                     print(dicts_to_compare[0][\"view\"][0][\"mark\"][0][\"encoding\"])\n",
    "# #                     print(dicts_to_compare[1][\"view\"][0][\"mark\"][0][\"encoding\"])\n",
    "#                     print(content_json)\n",
    "#                     print(dicts_to_compare[0])\n",
    "#                     print(dicts_to_compare[1])\n",
    "#                     print(content_json[\"view\"] == dicts_to_compare[0][\"view\"] or content_json[\"view\"] == dicts_to_compare[1][\"view\"])\n",
    "            \n",
    "                if compare_dicts(dicts_to_compare[0], content_json):\n",
    "                    removed += 1\n",
    "                    del pairs[0]\n",
    "                if compare_dicts(dicts_to_compare[1], content_json):\n",
    "                    removed += 1\n",
    "                    del pairs[1]\n",
    "\n",
    "                    \n",
    "                    \n",
    "                with open(training_data_path, 'a') as training_file:\n",
    "                    for j, pair in enumerate(pairs[:2]):\n",
    "                        data_to_append = json.dumps({\n",
    "                            \"task\": task,\n",
    "                            \"positive\": positive_facts,\n",
    "                            \"negative\": pair,\n",
    "                            \"significant\": \"\"\n",
    "                        })\n",
    "                        training_file.write(data_to_append + \",\\n\")\n",
    "                        \n",
    "#                         os.makedirs(os.path.join(gpt_responses_dir, \"kim2018_for_reader\", f\"{model}_{temperature}\"), exist_ok=True)\n",
    "#                         print(num)\n",
    "#                         #write to individual txt files and show diff in feature vectors\n",
    "#                         with open(os.path.join(gpt_responses_dir, \"kim2018_for_reader\", f\"{model}_{temperature}\", f\"{num}.txt\"), 'w') as f:\n",
    "#                             f.write(str(content))\n",
    "#                             f.write(\"\\n\")\n",
    "#                             f.write(str(json.loads(chart[9:])))\n",
    "\n",
    "#                             temp = {}\n",
    "#                             pair = json.loads(data_to_append)\n",
    "#                             pair[\"source\"] = \"kim\"\n",
    "#                             pair[\"pair_id\"] = \"kim_0\"\n",
    "#                             temp[\"kim_0\"] = pair\n",
    "\n",
    "#                             baseline_train_data = pairs_to_vec(temp)\n",
    "#                             diff = baseline_train_data.positive - baseline_train_data.negative\n",
    "#                             non_zero_columns = diff.iloc[0][diff.iloc[0] != 0]\n",
    "#                             f.write('\\n\\n' + non_zero_columns.to_string() + \"\\n\\n\")\n",
    "#                             print(num, non_zero_columns.to_string())\n",
    "\n",
    "\n",
    "\n",
    "    with open(training_data_path, 'a') as training_file:\n",
    "        training_file.write(\"\\n]\")                    \n",
    "                    \n",
    "            \n",
    "    percent = removed / (2 * (len(os.listdir(optimal_completion_dir)) - err))\n",
    "    print(f\"Total {removed} chart pairs removed, {percent} removed out of {2 * (len(os.listdir(optimal_completion_dir)) - err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1356e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 474 chart pairs removed, 0.22170252572497662 removed out of 2138\n",
      "The file has been successfully corrected and saved.\n"
     ]
    }
   ],
   "source": [
    "gpt_responses_dir = \"./gpt_responses\"\n",
    "optimal_completion_dir = \"./to_dict_gpt_responses/kim2018_responses/gpt-4-0125-preview_0\"\n",
    "original_pairs_dir = \"../rank/data/example_pairs_to_rank/kim2018\"\n",
    "training_data_path = \"./training_data/kim2018/gpt-4-0125-preview_0_training.json\"\n",
    "write_dicts_to_json_training_file(\"gpt-4-0125-preview\", 0)\n",
    "try:\n",
    "    # Read the content of the file\n",
    "    \n",
    "    with open(training_data_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Finding the last comma in the file\n",
    "    last_comma_index = content.rfind(',')\n",
    "\n",
    "    # Removing the last comma\n",
    "    if last_comma_index != -1:\n",
    "        content = content[:last_comma_index] + content[last_comma_index + 1:]\n",
    "    \n",
    "    # Writing the corrected content back to the file\n",
    "    with open(training_data_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print(\"The file has been successfully corrected and saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd5765d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 504 chart pairs removed, 0.22971741112123975 removed out of 2194\n",
      "The file has been successfully corrected and saved.\n"
     ]
    }
   ],
   "source": [
    "gpt_responses_dir = \"./gpt_responses\"\n",
    "optimal_completion_dir = \"./to_dict_gpt_responses/kim2018_responses/altair_gpt-4-0125-preview_0\"\n",
    "original_pairs_dir = \"../rank/data/example_pairs_to_rank/kim2018\"\n",
    "training_data_path = \"./training_data/kim2018/altair_gpt-4-0125-preview_0_training.json\"\n",
    "write_dicts_to_json_training_file(\"altair_gpt-4-0125-preview\", 0)\n",
    "try:\n",
    "    # Read the content of the file\n",
    "    \n",
    "    with open(training_data_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Finding the last comma in the file\n",
    "    last_comma_index = content.rfind(',')\n",
    "\n",
    "    # Removing the last comma\n",
    "    if last_comma_index != -1:\n",
    "        content = content[:last_comma_index] + content[last_comma_index + 1:]\n",
    "    \n",
    "    # Writing the corrected content back to the file\n",
    "    with open(training_data_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print(\"The file has been successfully corrected and saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce5b41df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 464 chart pairs removed, 0.22700587084148727 removed out of 2044\n",
      "The file has been successfully corrected and saved.\n"
     ]
    }
   ],
   "source": [
    "gpt_responses_dir = \"./gpt_responses\"\n",
    "optimal_completion_dir = \"./to_dict_gpt_responses/kim2018_responses/gpt-4-0613_0\"\n",
    "original_pairs_dir = \"../rank/data/example_pairs_to_rank/kim2018\"\n",
    "training_data_path = \"./training_data/kim2018/gpt-4-0613_0_training.json\"\n",
    "write_dicts_to_json_training_file(\"gpt-4-0613\",0)\n",
    "try:\n",
    "    # Read the content of the file\n",
    "    \n",
    "    with open(training_data_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Finding the last comma in the file\n",
    "    last_comma_index = content.rfind(',')\n",
    "\n",
    "    # Removing the last comma\n",
    "    if last_comma_index != -1:\n",
    "        content = content[:last_comma_index] + content[last_comma_index + 1:]\n",
    "    \n",
    "    # Writing the corrected content back to the file\n",
    "    with open(training_data_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print(\"The file has been successfully corrected and saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f20b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 452 chart pairs removed, 0.23203285420944558 removed out of 1948\n",
      "The file has been successfully corrected and saved.\n"
     ]
    }
   ],
   "source": [
    "gpt_responses_dir = \"./gpt_responses\"\n",
    "optimal_completion_dir = \"./to_dict_gpt_responses/kim2018_responses/altair_gpt-4-0613_0\"\n",
    "original_pairs_dir = \"../rank/data/example_pairs_to_rank/kim2018\"\n",
    "training_data_path = \"./training_data/kim2018/altair_gpt-4-0613_0_training.json\"\n",
    "write_dicts_to_json_training_file(\"altair_gpt-4-0613\", 0)\n",
    "try:\n",
    "    # Read the content of the file\n",
    "    \n",
    "    with open(training_data_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Finding the last comma in the file\n",
    "    last_comma_index = content.rfind(',')\n",
    "\n",
    "    # Removing the last comma\n",
    "    if last_comma_index != -1:\n",
    "        content = content[:last_comma_index] + content[last_comma_index + 1:]\n",
    "    \n",
    "    # Writing the corrected content back to the file\n",
    "    with open(training_data_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print(\"The file has been successfully corrected and saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "297ea742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 47 chart pairs removed, 0.10681818181818181 removed out of 440\n",
      "The file has been successfully corrected and saved.\n"
     ]
    }
   ],
   "source": [
    "gpt_responses_dir = \"./gpt_responses\"\n",
    "optimal_completion_dir = \"./to_dict_gpt_responses/kim2018_responses/gpt-3.5-turbo-0125_0\"\n",
    "original_pairs_dir = \"../rank/data/example_pairs_to_rank/kim2018\"\n",
    "training_data_path = \"./training_data/kim2018/gpt-3.5-turbo-0125_0_training.json\"\n",
    "write_dicts_to_json_training_file(\"gpt-3.5-turbo-0125\", 0)\n",
    "try:\n",
    "    # Read the content of the file\n",
    "    \n",
    "    with open(training_data_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Finding the last comma in the file\n",
    "    last_comma_index = content.rfind(',')\n",
    "\n",
    "    # Removing the last comma\n",
    "    if last_comma_index != -1:\n",
    "        content = content[:last_comma_index] + content[last_comma_index + 1:]\n",
    "    \n",
    "    # Writing the corrected content back to the file\n",
    "    with open(training_data_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print(\"The file has been successfully corrected and saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcddc86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains valid JSON.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./training_data/kim2018/gpt-4-1106-preview_0_training.json\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Attempt to load the JSON content\n",
    "        data = json.load(file)\n",
    "    print(\"The file contains valid JSON.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"The file contains invalid JSON.\")\n",
    "    print(\"Error details:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ad0d8-e5b6-4029-a96f-631d54b40bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
